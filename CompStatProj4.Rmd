---
title: "Statistical Computing Project 4"
author: "Mario Ibanez"
date: "March 26, 2016"
output: pdf_document
---

# Introduction
The purpose of this report is to evaluate the effectiveness of the EM algorithm in solving a mixture model problem.  The problem that will be solved in this report is that given a data set, attempt to estimate the proportion at which data was sampled from two normal distributions along with estimating the means and standard deviations of those two distributions.  More explicitly, the data will be sampled from the distribution:

$$
f(x) = w_1N(\mu_1, \sigma_1) + w_2N(\mu_2, \sigma_2) = w_1f_1(x) + w_2f_2(x)
$$

and the EM algorithm will be used to estimate the parameters $w_1$, $w_2$, $\mu_1$, $\mu_2$, $\sigma_1$ and $\sigma_2$.  In addition to estimating these parameters, an attempt can be made at evaluating the algorithm itself by calculating various statistics reflecting the algorithm's performance.

# Methods

In order to evaluate the algorithm, known values for the parameters listed above will be used to generate data.  The algorithm will then use this data to estimate the six unknown mixture model parameters.  However, because of the constraint that $w_1 + w_2 = 1$, there are in fact only five unknown parameters.  If the true values for these parameters are known, the algorithm can be evaluated for effectiveness by comparing the estimates it generates to the true values.  A method that will be used to evaluate the accuracy of the algorithm will be "z-scores" for the estimated means.  Explicitly:

$$
z_1 = \frac{\hat{\mu_1} - \mu_1}{\sigma_1} \mbox{    and    } z_2 = \frac{\hat{\mu_2} - \mu_2}{\sigma_2}
$$

These z-scores will measure how far away from the true values the estimates of the means were.  

Though there are different approaches possible in order to evaluate the EM algorithm's ability to solve a mixture model problem, the one used in this report is to run the algorithm repeatedly for different samples from the same distribution.  This will give an idea, for that specific distribution, how the algorithm tends to perform.  Another alternative approach that will not be taken in this report is to run the algorithm a single time for different distributions and compare the algorithm's ability to solve the problem depending on the parameters.

## Data

The data used in this report to evaluate the EM algorithm will be 100 values sampled from the following mixture distribution:

$$
f(x) = (0.30)N(-1, 2) + (0.70)N(2, 1)
$$

In order to perform this sampling, a random number will be sampled from the binomial distribution with parameters $n=100$ and $p=0.30$.  This value will be used to determine how many values are sampled from $N(-1, 2)$.  The rest of the values will then be sampled from $N(2, 1)$.  Sampling from a normal distribution with any parameters is easy in the *R* language using a built in function.  The code below generates the data:

```{r}
# Mixture weights, seed, and total sample size
proportion <- 0.30
sample_size <- 100
set.seed(1234)

# Means and standard deviations
mu_1 <- -1; mu_2 <- 2
sigma_1 <- 2; sigma_2 <- 1

# Calculate how many values will come from each component
sample1_size <- rbinom(n = 1, size = sample_size, prob = proportion)
sample2_size <- sample_size - sample1_size

# Sample that many values from each respective normal distribution
sample1 <- rnorm(n = sample1_size, mean = mu_1, sd = sigma_1)
sample2 <- rnorm(n = sample2_size, mean = mu_2, sd = sigma_2)
```

The plot below allows us to visualize the data set that will be used later in the report to evaluate the EM algorithm:

$\smallskip$

```{r echo = FALSE}
x_axis <- seq(from = min(sample1), to = max(sample2), by = 0.1)

# Plot distribution of full sample
plot(x = density(c(sample1, sample2)),
     ylim = c(-0.01, .35),
     lwd = 2, main = "Random Data")

# Plot true population distribution
lines(x = x_axis, 
      y = (proportion * dnorm(x = x_axis, mean = mu_1, sd = sigma_1)) + 
          (1 - proportion)*dnorm(x = x_axis, mean = mu_2, sd = sigma_2),
      lty = 2, lwd = 2)

# Plot distribution of sample 1
points(x = sample1,
       y = rep(0, sample1_size), 
       col = "red")

# Plot true normal N(-1, 2)
lines(x = x_axis, 
      y = proportion * dnorm(x = x_axis, mean = mu_1, sd = sigma_1), 
      col = "red",
      lty = 2, lwd = 2)

# Plot distribution of sample 2
points(x = sample2,
       y = rep(-0.005, sample2_size), 
       col = "blue")

# Plot true normal N(2, 1)
lines(x = x_axis, 
      y = (1 - proportion)*dnorm(x = x_axis, mean = mu_2, sd = sigma_2), 
      col = "blue",
      lty = 2, lwd = 2)
```

$\smallskip$

In the plot, the dotted lines represent the true population distributions.  The red dotted line is the PDF of $N(-1, 2)$ while the blue dotted line is the PDF of $N(2, 1)$ and the black dotted line is the PDF of $(0.30)N(-1, 2) + (0.70)N(2, 1)$.  The red and blue points at the bottom of the plot represent the generated data.  Finally, the black solid line is a plot of the density of the generated data.  Notice of course that while the black dotted line and the black solid line should be similar in shape, there is no guarantee that from sample to sample this will occur.  This is especially true when the two distributions significantly overlap as they do in the example in this report.

# EM Algorithm

The EM algorithm works by alternating between two steps until convergence.  The first step is the Expectation step and the second step is the Maximization step.  In this applicaton of the algorithm, the expectation step involves the relative probability that a data point was generated from the first distribution. (Note, the first distribution is $f_1(x) = N(-1, 2)$)  This value is calcuated for each data point in each iteration.  The formula for the $i^{th}$ value is:

$$
\hat{z_i} = \frac{\hat{w_1} f_1(x_i)}{\hat{w_1} f_1(x_i) + (1- \hat{w_1})f_2(x_i)}
$$

This value is used to weight the estimates for the parameters $\mu_1$, $\mu_2$, $\sigma_1$, and $\sigma_2$.  The next step is the maximization step.  Updated estimates of the five parameters are calculated:

$$
\hat{\mu_1} = \frac{\sum \hat{z_i} x_i}{\sum \hat{z_i}} \mbox{  and   } \hat{\mu_2} = \frac{\sum (1 - \hat{z_i}) x_i}{\sum (1 - \hat{z_i})}
$$

$$
\hat{\sigma_1^2} = \frac{\sum \hat{z_i} (x_i - \hat{\mu_1})^2}{\sum \hat{z_i}} \mbox{  and   } \hat{\sigma_2^2} = \frac{\sum (1 - \hat{z_i}) (x_i - \hat{\mu_2})^2}{\sum (1 - \hat{z_i})}
$$

$$
\hat{w_1} = \frac{\sum \hat{z_i}}{N} \mbox{  where } N = 100
$$

Written more explictily, the steps are:  
1) Initialize the five unknown paramater values with guesses  
2) Calculate $\hat{z_i}$ for each value  
3) Calculate new esimates for the five parameters  
4) Return to step 2 if convergence criteria has not been met   

The algorithm will run until the difference between to successive estimates of $\mu_1$ differ by less than 0.01.  This is an arbitrary convergence criteria but will serve fine for the purpose of this report.


# Simulation

As was partly mentioned earlier in the paper, the EM algorithm will be evaluated by repeatedly generating new random data from the distribution:

$$
f(x) = (0.30)N(-1, 2) + (0.70)N(2, 1)
$$

The "z-scores" defined earlier will be calculated each time.  Random data will be generated 500 times and the algorithm allowed to run until reaching its convergence criteria each time.  The initial values for the parameters will be the same in each trial: $\mu_1 = -0.5$, $\mu_2 = 0.5$, $\sigma_1 = 1$, $\sigma_2 = 1$ and $w_1 = 0.5$.  The simulations will be done in *R* by using a loop to repeatedly generate random data, rerun the algorithm, and store the z scores for that trial.  The code in the appendix accomplishes this.

$\pagebreak$

```{r echo=FALSE}
# Keep track of z-scores and number of iterations until convergence
results <- data.frame(mu1 = rep(0, 500),
                      mu2 = rep(0, 500),
                      sigma1 = rep(0, 500),
                      sigma2 = rep(0, 500),
                      prop = rep(0, 500),
                      z1 = rep(0, 500),
                      z2 = rep(0, 500),
                      iterations = rep(0, 500))

# Run EM algorithm 500 times on same problem
for(i in 1:500){
  # Initial parameter values guesses
  mu1_hat <- -0.5
  mu2_hat <- 0.5
  sig1_hat <- 1
  sig2_hat <- 1
  weight <- 0.5
  
  # Calculate how many values will come from each component
  sample1_size <- rbinom(n = 1, size = sample_size, prob = proportion)
  sample2_size <- sample_size - sample1_size

  # Sample that many values from each respective normal distribution
  sample1 <- rnorm(n = sample1_size, mean = mu_1, sd = sigma_1)
  sample2 <- rnorm(n = sample2_size, mean = mu_2, sd = sigma_2)
  sample <- c(sample1, sample2)
  
  # number of iterations
  iterations <- 0
  
  while(TRUE){
    # increment iterations
    iterations <- iterations + 1
    
    # E step
    z_hat <- (weight * dnorm(sample, mu1_hat, sig1_hat)) / 
      ((weight * dnorm(sample, mu1_hat, sig1_hat)) 
       + (1 - weight) * dnorm(sample, mu2_hat, sig2_hat))
    
    # M step
    m_mu1_hat <- sum(z_hat * sample) / sum(z_hat)
    m_mu2_hat <- sum((1 - z_hat) * sample) / sum(1 - z_hat)
    m_sig1_hat <- sqrt(sum(z_hat * (sample - mu1_hat)^2) / sum(z_hat))
    m_sig2_hat <- sqrt(sum((1 - z_hat) * (sample - mu2_hat)^2) / sum(1 - z_hat))
    m_weight <- sum(z_hat) / 500
    
    # Stopping condition
    if(abs(mu1_hat - m_mu1_hat) < 0.01){
      # store values and break loop
      results$mu1[i] <- m_mu1_hat
      results$mu2[i] <- m_mu2_hat
      results$sigma1[i] <- m_sig1_hat
      results$sigma2[i] <- m_sig2_hat
      results$prop[i] <- m_weight
      results$z1[i] <- (m_mu1_hat - mu_1) / sigma_1
      results$z2[i] <- (m_mu2_hat - mu_2) / sigma_2
      results$iterations[i] <- iterations
      break
    } else{ # update values and continue loop
      mu1_hat <- m_mu1_hat; mu2_hat <- m_mu2_hat
      sig1_hat <- m_sig1_hat; sig2_hat <- m_sig2_hat
      weight <- m_weight
    }
  }
}
```

Next, the results of the simulation are analyzed.   

# Results

Again, the simulation repeated the EM algorithm 500 times and each time various values were stored.  We can look at these results value by value to understand what occured.  

## Estimate of $\mu_1$

The true value of $\mu_1$ is -1.  The concern with this parameter is that since this distribution is being sampled from with a weight of 0.30 and has a high standard deviation of 2, it can be difficult to detect its true value.  The plot below shows a distribution of the estimates $\hat{mu_1}$:

$\smallskip$

```{r echo=FALSE}
plot(density(results$mu1), main = expression(paste("Distribution of ", hat(mu[1]))))
```

$\smallskip$

We see that the algorithm totally fails to detect that the true value was -1.  We can also look at how much variablility there was in the estimates by looking at the distribution of the "z-scores" that was defined earlier.

$\smallskip$

```{r echo=FALSE}
plot(density(results$z1), main = expression(paste("Distribution of ", z[1])))
```

$\smallskip$

This distribution looks very similar to the distribution of $\hat{\mu_1}$.  In fact this is the same plot shifted by 2.

## Estimate of $\mu_2$

The true value of $\mu_2$ is 2.  As was mentioned earlier, the concern is that with the parameter values in this example causing a significant overlap between the two distributions, it may be difficult to detect the true parameter values.  The plot below shows a distribution of the estimates $\hat{mu_2}$:

$\smallskip$

```{r echo=FALSE}
plot(density(results$mu2), main = expression(paste("Distribution of ", hat(mu[2]))))
```

$\smallskip$

We see that the algorithm also fails to detect that the true mean is 2.  The plot below shows a distribution of the the "z-scores" for the estimates.  Most of the estimates were underestimates by about 1.

$\smallskip$

```{r echo=FALSE}
plot(density(results$z2), main = expression(paste("Distribution of ", z[2])))
```

$\smallskip$

Again, this distribution looks very similar to the distribution of $\hat{\mu_2}$.

## Estimate of the proportion

Before looking at the estiamtes for the standard deviations, the estimates for the proportions can be analyzed.  The true value is 0.30.  Below is the distribution of the proportion estiamtes:

$\smallskip$

```{r echo=FALSE}
hist(results$prop, main = expression(paste("Distribution of ", hat(w))), 
     breaks = seq(0, 0.07, by = 0.01),
     xlim = c(0, 0.07), xlab = "Proportion")
```

$\smallskip$

With the particular initial guesses that were used and the particular true values that were used, it seems the the algorithm outputs that approximately zero of the weight is placed on the first component of the mixture and that approximately all of the values are being generated from the second component of the mixture.  This is probably due to the fact that only approximately 30% of the values are being generated from the first mixture component, and this component is very close to the second component.   

## Distribution of number of iterations

Below is a histogram of the number of iterations needed for the algorithm to converge in the 500 trials:

$\smallskip$

```{r echo=FALSE}
hist(results$iterations, main = "Distribution of number of iterations", xlab = "Iterations")
```

$\smallskip$

The majority of trials required less than 30 iterations in order to converge.  

# Conclusion

Repeating the algorithm 500 times, the average values of the estimates across all 500 trials were:  

|Value                        |Mean Estimate|
|:----------------------------|------------:|
|$\overline{\hat{\mu_1}}$     |   -4.6289319|
|$\overline{\hat{\mu_2}}$     |    1.2261107|
|$\overline{\hat{\sigma_1}}$  |    0.2783892|
|$\overline{\hat{\sigma_2}}$  |    1.7644620|
|$\overline{\hat{w}}$         |    0.0052167|
|Iterations                   |   13.2160000|

Due to the two mixture components being somewhat near each other and due to the component with the larger variance also being the component with the smaller weight or proportion, the EM algorithm estimated its mean to be further to the left than its true value and vastly underestimated its weight.  There are other possible ways to test and analyze the EM algorithm, but this example in this report shows what occurs when the two components are too close to each other and the weight of one component is much less than the other.  The EM algorithm is not easily able to give good approximations for the true values.  If the mixtures were far apart from one another, the algorithm would have a more successful outcome.